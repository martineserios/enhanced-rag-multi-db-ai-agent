{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chat Workflow Demo\n",
    "This notebook demonstrates the chat workflow by importing the necessary classes and methods from the project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Import required libraries\n",
    "import os\n",
    "import sys\n",
    "import asyncio\n",
    "from dotenv import load_dotenv\n",
    "from pathlib import Path\n",
    "\n",
    "# Add project root to path\n",
    "project_root = str(Path.cwd().parent.parent.absolute())\n",
    "if project_root not in sys.path:\n",
    "    sys.path.append(project_root)\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv(Path(project_root) / \".env\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Import project modules\n",
    "from app.config import get_settings, Settings\n",
    "from app.services.llm.factory import get_llm_service\n",
    "from app.services.memory.manager import get_memory_manager\n",
    "from app.api.models.chat import ChatRequest, ChatResponse\n",
    "from app.core.logging import setup_logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Initialize settings and logging\n",
    "settings = get_settings()\n",
    "setup_logging(level=settings.get_log_level(), json_format=settings.log_json)\n",
    "\n",
    "print(f\"Using LLM provider: {settings.llm_provider}\")\n",
    "print(f\"Model: {settings.llm_model}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "async def initialize_services():\n",
    "    \"\"\"Initialize LLM and memory services.\"\"\"\n",
    "    # Initialize LLM service\n",
    "    llm_service = get_llm_service(\n",
    "        provider=settings.llm_provider,\n",
    "        model=settings.llm_model,\n",
    "        temperature=settings.llm_temperature\n",
    "    )\n",
    "    \n",
    "    # Initialize memory manager\n",
    "    memory_manager = get_memory_manager(settings)\n",
    "    \n",
    "    return llm_service, memory_manager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "async def chat_workflow(message: str, conversation_id: str = None, llm_service=None, memory_manager=None):\n",
    "    \"\"\"\n",
    "    Mimic the chat workflow from the API.\n",
    "    \n",
    "    Args:\n",
    "        message: User message\n",
    "        conversation_id: Optional conversation ID for continuing a conversation\n",
    "        llm_service: Initialized LLM service\n",
    "        memory_manager: Initialized memory manager\n",
    "        \n",
",
    "    Returns:\n",
    "        Response from the LLM\n",
    "    \"\"\"\n",
    "    if conversation_id is None:\n",
    "        conversation_id = str(uuid.uuid4())\n",
    "    \n",
    "    # Create chat request\n",
    "    chat_request = ChatRequest(\n",
    "        message=message,\n",
    "        conversation_id=conversation_id,\n",
    "        stream=False\n",
    "    )\n",
    "    \n",
    "    try:\n",
    "        # Get relevant context from memory\n",
    "        context = await memory_manager.retrieve_memory(\n",
    "            query=message,\n",
    "            conversation_id=conversation_id,\n",
    "            limit=3\n",
    "        )\n",
    "        \n",
    "        # Generate response using LLM\n",
    "        response = await llm_service.generate(\n",
    "            prompt=message,\n",
    "            context=context,\n",
    "            conversation_id=conversation_id\n",
    "        )\n",
    "        \n",
    "        # Store interaction in memory\n",
    "        await memory_manager.store_memory(\n",
    "            memory_type=\"conversation\",\n",
    "            content={\"user\": message, \"assistant\": response},\n",
    "            key=f\"{conversation_id}:{int(time.time())}\",\n",
    "            metadata={\"type\": \"conversation\"},\n",
    "            conversation_id=conversation_id\n",
    "        )\n",
    "        \n",
    "        return response\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error in chat workflow: {str(e)}\")\n",
    "        return f\"Error: {str(e)}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Example usage\n",
    "async def main():\n",
    "    # Initialize services\n",
    "    llm_service, memory_manager = await initialize_services()\n",
    "    \n",
    "    # Example conversation\n",
    "    messages = [\n",
    "        \"Hello, how are you?\",\n",
    "        \"What can you tell me about this project?\",\n",
    "        \"How does the memory system work?\"\n",
    "    ]\n",
    "    \n",
    "    # Start a new conversation\n",
    "    conversation_id = str(uuid.uuid4())\n",
    "    print(f\"Starting new conversation: {conversation_id}\")\n",
    "    \n",
    "    for msg in messages:\n",
    "        print(f\"\\nYou: {msg}\")\n",
    "        response = await chat_workflow(\n",
    "            message=msg,\n",
    "            conversation_id=conversation_id,\n",
    "            llm_service=llm_service,\n",
    "            memory_manager=memory_manager\n",
    "        )\n",
    "        print(f\"Assistant: {response}\")\n",
    "    \n",
    "    # Clean up\n",
    "    if hasattr(llm_service, 'close'):\n",
    "        await llm_service.close()\n",
    "    if hasattr(memory_manager, 'close'):\n",
    "        await memory_manager.close()\n",
    "\n",
    "# Run the example\n",
    "await main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notes\n",
    "\n",
    "1. This notebook provides a simplified version of the chat workflow from the API.\n",
    "2. Make sure your environment variables are properly set in the `.env` file.\n",
    "3. The notebook includes error handling and cleanup of resources.\n",
    "4. You can modify the example messages in the `main()` function to test different scenarios."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
